{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ind.citeseer.allx\n",
      "Shape: torch.Size([3327, 3703])\n"
     ]
    }
   ],
   "source": [
    "directory = './code/results/citeseer/'\n",
    "file_name = 'ind.citeseer.allx'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print(f\"File: {file_name}\")\n",
    "    if hasattr(data, 'shape'):\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "    else:\n",
    "        print(\"The loaded data does not have a 'shape' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError: 'ascii' codec can't decode byte 0x85 in position 16: ordinal not in range(128)\n"
     ]
    }
   ],
   "source": [
    "directory = './code/datasets/citeseer/citeseer/raw/'\n",
    "file_name = 'ind.citeseer.allx'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)  # Ensure binary mode is used\n",
    "        print(f\"File: {file_name}\")\n",
    "        if hasattr(data, 'shape'):\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "        else:\n",
    "            print(\"The loaded data does not have a 'shape' attribute.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"UnicodeDecodeError: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError: 'ascii' codec can't decode byte 0xa1 in position 12: ordinal not in range(128)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Specify the directory and file name\n",
    "directory = './code/datasets/pubmed/pubmed/raw/'\n",
    "file_name = 'ind.pubmed.allx'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Open the file in binary mode and load it using pickle\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)  # Ensure binary mode is used\n",
    "        print(f\"File: {file_name}\")\n",
    "        if hasattr(data, 'shape'):\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "        else:\n",
    "            print(\"The loaded data does not have a 'shape' attribute.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"UnicodeDecodeError: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36282/3471845853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the BitcoinAlpha dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bitcoinalpha'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./code/datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Access the processed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(\n",
    "        dataset: dict(help='name of the dataset', option='-d', choices=supported_datasets) = 'cora',\n",
    "        data_dir: dict(help='directory to store the dataset') = './datasets',\n",
    "        data_range: dict(help='min and max feature value', nargs=2, type=float) = (0, 1),\n",
    "        val_ratio: dict(help='fraction of nodes used for validation') = .25,\n",
    "        test_ratio: dict(help='fraction of nodes used for test') = .25,\n",
    "):\n",
    "    data = supported_datasets[dataset](root=os.path.join(data_dir, dataset))\n",
    "    data = RandomNodeSplit(split='train_rest', num_val=val_ratio, num_test=test_ratio)(data[0])\n",
    "    data = ToSparseTensor()(data)\n",
    "    data.name = dataset\n",
    "    data.num_classes = int(data.y.max().item()) + 1\n",
    "\n",
    "    if data_range is not None:\n",
    "        low, high = data_range\n",
    "        data = Normalize(low, high)(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the BitcoinAlpha dataset\n",
    "data = load_dataset(dataset='bitcoinalpha', data_dir='./code/datasets')\n",
    "\n",
    "# Access the processed data\n",
    "print(data)\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.edge_index.size(1)}\")\n",
    "print(f\"Feature matrix shape: {data.x.shape}\")\n",
    "print(f\"Labels shape: {data.y.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the bitcoinalpha dataset\n",
    " def process(self):\n",
    "        import networkx as nx\n",
    "        import numpy as np\n",
    "\n",
    "        file_path = os.path.join(self.raw_dir, self.raw_file_names[0])\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Step 1: Map nodes to a continuous ID range\n",
    "        unique_nodes = pd.unique(df[['SOURCE', 'TARGET']].values.ravel())\n",
    "        node_map = {node: idx for idx, node in enumerate(unique_nodes)}\n",
    "        num_nodes = len(node_map)\n",
    "\n",
    "        df['SOURCE'] = df['SOURCE'].map(node_map)\n",
    "        df['TARGET'] = df['TARGET'].map(node_map)\n",
    "\n",
    "        # Step 2: Build graph from remapped edges\n",
    "        G = nx.from_pandas_edgelist(df, source='SOURCE', target='TARGET', edge_attr='RATING', create_using=nx.DiGraph())\n",
    "        edge_index = torch.tensor(list(G.edges), dtype=torch.long).t()\n",
    "\n",
    "        # Step 3: Generate features and labels\n",
    "        max_rating = max(df['RATING'].abs())\n",
    "        dmax_in = max(dict(G.in_degree()).values())\n",
    "        dmax_out = max(dict(G.out_degree()).values())\n",
    "\n",
    "        y = []\n",
    "        x = []\n",
    "\n",
    "        for node in range(num_nodes):\n",
    "            in_edges = list(G.in_edges(node))\n",
    "            out_edges = list(G.out_edges(node))\n",
    "\n",
    "            # Label\n",
    "            if len(in_edges) < 3:\n",
    "                label = 0\n",
    "            else:\n",
    "                rating_sum = sum(G[u][v]['RATING'] / abs(G[u][v]['RATING']) for u, v in in_edges)\n",
    "                avg_rating = rating_sum / len(in_edges)\n",
    "                label = 1 if avg_rating >= 0.3 else 0\n",
    "            y.append(label)\n",
    "\n",
    "            # Features\n",
    "            if len(out_edges) == 0:\n",
    "                features = np.ones(8) / 1000\n",
    "            else:\n",
    "                w_pos = sum(G[u][v]['RATING'] for u, v in out_edges if G[u][v]['RATING'] > 0)\n",
    "                w_neg = -sum(G[u][v]['RATING'] for u, v in out_edges if G[u][v]['RATING'] < 0)\n",
    "                abstotal = w_pos + w_neg\n",
    "                avg = (w_pos - w_neg) / len(out_edges) / max_rating\n",
    "\n",
    "                f = np.zeros(8)\n",
    "                f[0] = w_pos / max_rating / len(out_edges)\n",
    "                f[1] = w_neg / max_rating / len(out_edges)\n",
    "                f[2] = w_pos / abstotal if abstotal else 0\n",
    "                f[3] = avg\n",
    "                f[4] = f[0] * G.in_degree(node) / dmax_in if dmax_in else 0\n",
    "                f[5] = f[1] * G.in_degree(node) / dmax_in if dmax_in else 0\n",
    "                f[6] = f[0] * G.out_degree(node) / dmax_out if dmax_out else 0\n",
    "                f[7] = f[1] * G.out_degree(node) / dmax_out if dmax_out else 0\n",
    "                features = f / 1.01 + 0.001\n",
    "\n",
    "            x.append(features)\n",
    "\n",
    "        data = Data(\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor(y, dtype=torch.long),\n",
    "            num_nodes=num_nodes\n",
    "        )\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
